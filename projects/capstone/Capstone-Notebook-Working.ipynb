{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "import eli5\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, log_loss\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "def weighStats(row, dayWeight, locWeight):\n",
    "    weight = 1 \n",
    "    if row['DayNum'] > (132 * 0.75):\n",
    "        weight += dayWeight\n",
    "    if (row['WLoc'] == \"A\") or (row['WLoc'] == \"N\"):\n",
    "        weight += locWeight\n",
    "    # print(row)\n",
    "    \n",
    "    # Weighting winner stats\n",
    "    row['Wsos'] = row['Wsos'] * weight\n",
    "    row['Wposs'] = row['Wposs'] * weight\n",
    "    row['Wshoot_eff'] = row['Wshoot_eff'] * weight \n",
    "    row['Wscore_op'] = row['Wscore_op'] * weight \n",
    "    row['Woff_rtg'] = row['Woff_rtg'] * weight \n",
    "    row['Wdef_rtg'] = row['Wdef_rtg'] * weight \n",
    "    row['Wts_pct'] = row['Wts_pct'] * weight \n",
    "    row['Wefg_pct'] = row['Wefg_pct'] * weight \n",
    "    row['Worb_pct'] = row['Worb_pct'] * weight \n",
    "    row['Wdrb_pct'] = row['Wdrb_pct'] * weight \n",
    "    row['Wreb_pct'] = row['Wreb_pct'] * weight \n",
    "    row['Wto_poss'] = row['Wto_poss'] * weight \n",
    "    row['Wft_rate'] = row['Wft_rate'] * weight \n",
    "    row['Wast_rtio'] = row['Wast_rtio'] * weight\n",
    "    row['Wblk_pct'] = row['Wblk_pct'] * weight\n",
    "    row['Wstl_pct'] = row['Wstl_pct'] * weight\n",
    "    \n",
    "    # Weighting Losses\n",
    "#     row['Lposs'] = row['Lposs'] * weight\n",
    "#     row['Lshoot_eff'] = row['Lshoot_eff'] * weight\n",
    "#     row['Lscore_op'] = row['Lscore_op'] * weight\n",
    "#     row['Loff_rtg'] = row['Loff_rtg'] * weight\n",
    "#     row['Ldef_rtg'] = row['Ldef_rtg'] * weight\n",
    "#     row['Lts_pct'] = row['Lts_pct'] * weight\n",
    "#     row['Lefg_pct'] = row['Lefg_pct'] * weight\n",
    "#     row['Lorb_pct'] = row['Lorb_pct'] * weight\n",
    "#     row['Ldrb_pct'] = row['Ldrb_pct'] * weight\n",
    "#     row['Lreb_pct'] = row['Lreb_pct'] * weight\n",
    "#     row['Lto_poss'] = row['Lto_poss'] * weight\n",
    "#     row['Lft_rate'] = row['Lft_rate'] * weight\n",
    "#     row['Last_rtio'] = row['Last_rtio'] * weight\n",
    "#     row['Lblk_pct'] = row['Lblk_pct'] * weight\n",
    "#     row['Lstl_pct'] = row['Lstl_pct'] * weight\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "def calcAverages(df):\n",
    "    df_avg = pd.DataFrame()\n",
    "\n",
    "    df_avg['n_wins'] = df['WTeamID'].groupby([df.Season, df.WTeamID, df.WTeamName, df.WConfName]).count()\n",
    "    df_avg['n_loss'] = df['LTeamID'].groupby([df.Season, df.LTeamID, df.LTeamName, df.LConfName]).count()\n",
    "\n",
    "    df_avg['n_loss'].fillna(0, inplace=True)\n",
    "\n",
    "    # Calculate win percentages:\n",
    "    df_avg['win_pct'] = df_avg['n_wins'] / (df_avg['n_wins'] + df_avg['n_loss'])\n",
    "    # Calculate averages for games won:\n",
    "    df_avg['Wshoot_eff'] = df['Wshoot_eff'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Wscore_op'] = df['Wscore_op'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Woff_rtg'] = df['Woff_rtg'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Wdef_rtg'] = df['Wdef_rtg'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Wsos'] = df['Wsos'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Wts_pct'] = df['Wts_pct'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Wefg_pct'] = df['Wefg_pct'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Worb_pct'] = df['Worb_pct'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Wdrb_pct'] = df['Wdrb_pct'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Wreb_pct'] = df['Wreb_pct'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Wto_poss'] = df['Wto_poss'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Wft_rate'] = df['Wft_rate'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Wie'] = df['Wie'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Wast_rtio'] = df['Wast_rtio'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Wblk_pct'] = df['Wblk_pct'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Wstl_pct'] = df['Wstl_pct'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "\n",
    "    # Calculate averages for games lost:\n",
    "    df_avg['Lshoot_eff'] = df['Lshoot_eff'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Lscore_op'] = df['Lscore_op'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Loff_rtg'] = df['Loff_rtg'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Ldef_rtg'] = df['Ldef_rtg'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Lsos'] = df['Lsos'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Lts_pct'] = df['Lts_pct'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Lefg_pct'] = df['Lefg_pct'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Lorb_pct'] = df['Lorb_pct'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Ldrb_pct'] = df['Ldrb_pct'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Lreb_pct'] = df['Lreb_pct'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Lto_poss'] = df['Lto_poss'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Lft_rate'] = df['Lft_rate'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Lie'] = df['Lie'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Last_rtio'] = df['Last_rtio'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Lblk_pct'] = df['Lblk_pct'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "    df_avg['Lstl_pct'] = df['Lstl_pct'].groupby([df['Season'], df['WTeamID']]).mean()\n",
    "\n",
    "    return df_avg\n",
    "\n",
    "\n",
    "def get_year_t1_t2(ID):\n",
    "    \"\"\"Return a tuple with ints `year`, `team1` and `team2`.\"\"\"\n",
    "    return (int(x) for x in ID.split('_'))\n",
    "\n",
    "def predictMatches(clf, df_predict, df_features, year):\n",
    "    diff = []\n",
    "    data = []\n",
    "\n",
    "    for i, row in df_predict.iterrows():\n",
    "        year, team1, team2 = get_year_t1_t2(row.ID)\n",
    "#         print(\"Year: {}; Team1: {}; Team2: {}\".format(year, team1, team2))\n",
    "#         Save 2018 stats/features for the first ID:\n",
    "        team1 = df_features[(df_features['Season'] == year) & (df_features['TeamID'] == team1)].values[0]\n",
    "\n",
    "        # Save 2018 stats/features for the first ID:\n",
    "        team2 = df_features[(df_features['Season'] == year) & (df_features['TeamID'] == team2)].values[0]   \n",
    "\n",
    "        diff = team1 - team2\n",
    "\n",
    "        data.append(diff)\n",
    "\n",
    "    n_poss_games = len(df_predict)\n",
    "    columns = df_features.columns.get_values()\n",
    "#     print(columns)\n",
    "    final_predictions = pd.DataFrame(np.array(data).reshape(n_poss_games, np.array(data).shape[1]), columns=(columns))\n",
    "    final_predictions.drop(['Season', 'TeamID'], inplace=True, axis=1)\n",
    "    predictions = clf.predict_proba(final_predictions)[:, 1]\n",
    "#     predLogLoss = log_loss(y_test, search.predict_proba(final_predictions))\n",
    "    clipped_predictions = np.clip(predictions, 0.05, 0.95)\n",
    "    df_predict.Pred = clipped_predictions\n",
    "    \n",
    "    return df_predict\n",
    "\n",
    "\n",
    "# Source: https://www.kaggle.com/goodspellr/how-to-score-your-own-predictions-and-more\n",
    "def kaggle_clip_log(x):\n",
    "    '''\n",
    "    Calculates the natural logarithm, but with the argument clipped within [1e-15, 1 - 1e-15]\n",
    "    '''\n",
    "    return np.log(np.clip(x,1.0e-15, 1.0 - 1.0e-15))\n",
    "\n",
    "def kaggle_log_loss(pred, result):\n",
    "    '''\n",
    "    Calculates the kaggle log loss for prediction pred given result result\n",
    "    '''\n",
    "    return -(result*kaggle_clip_log(pred) + (1-result)*kaggle_clip_log(1.0 - pred))\n",
    "    \n",
    "def score_submission(df_sub, df_results, on_season=None, return_df_analysis=True):\n",
    "    '''\n",
    "    Scores a submission against relevant tournament results\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    df_sub: Pandas dataframe containing predictions to be scored (must contain a column called 'ID' and \n",
    "            a column called 'Pred')\n",
    "            \n",
    "    df_results: Pandas dataframe containing results to be compared against (must contain a column \n",
    "            called 'ID' and a column called 'Result')\n",
    "            \n",
    "    on_season: array-like or None.  If array, should contain the seasons for which a score should\n",
    "            be calculated.  If None, will use all seasons present in df_results\n",
    "            \n",
    "    return_df_analysis: Bool.  If True, will return the dataframe used for calculations.  This is useful\n",
    "            for future analysis\n",
    "            \n",
    "    Returns\n",
    "    =======\n",
    "    df_score: pandas dataframe containing the average score over predictions that were scorable per season\n",
    "           as well as the number of obvious errors encountered\n",
    "    df_analysis:  pandas dataframe containing information about all results used in scoring\n",
    "                  Only provided if return_df_analysis=True\n",
    "    '''\n",
    "    \n",
    "    df_analysis = df_results.copy()\n",
    "    \n",
    "    # this will overwrite if there's already a season column but it should be the same\n",
    "    df_analysis['Season'] = [int(x.split('_')[0]) for x in df_results['ID']]\n",
    "    \n",
    "    if not on_season is None:\n",
    "        df_analysis = df_analysis[np.in1d(df_analysis['Season'], on_season)]\n",
    "        \n",
    "    # left merge with the submission.  This will keep all games for which there\n",
    "    # are results regardless of whether there is a prediction\n",
    "    df_analysis = df_analysis.merge(df_sub, how='left', on='ID')\n",
    "    \n",
    "    # check to see if there are obvious errors in the predictions:\n",
    "    # Obvious errors include predictions that are less than 0, greater than 1, or nan\n",
    "    # You can add more if you like\n",
    "    df_analysis['ObviousError'] = 1*((df_analysis['Pred'] < 0.0) \\\n",
    "                                  | (df_analysis['Pred'] > 1.0) \\\n",
    "                                  | (df_analysis['Pred'].isnull()))\n",
    "    \n",
    "    df_analysis['LogLoss'] = kaggle_log_loss(df_analysis['Pred'], df_analysis['Result'])\n",
    "    \n",
    "    df_score = df_analysis.groupby('Season').agg({'LogLoss' : 'mean', 'ObviousError': 'sum'})\n",
    "    \n",
    "    if return_df_analysis:\n",
    "        return df_score, df_analysis\n",
    "    else:\n",
    "        return df_score\n",
    "    \n",
    "def convertTourneyRound(dayNum):\n",
    "    \"\"\"\n",
    "    Consolidate tournament rounds into meaningful info.\n",
    "    \"\"\"\n",
    "    if (dayNum == 136) | (dayNum == 137):\n",
    "        return 64\n",
    "    elif (dayNum == 138) | (dayNum == 139):\n",
    "        return 32\n",
    "    elif (dayNum == 143) | (dayNum == 144):\n",
    "        return 16\n",
    "    elif (dayNum == 145) | (dayNum == 146):\n",
    "        return 8\n",
    "    elif dayNum == 152:\n",
    "        return 4\n",
    "    elif dayNum == 154:\n",
    "        return 2\n",
    "    else:\n",
    "        return 68\n",
    "    \n",
    "def processSeeds(df):\n",
    "    df['seed'] = df_tourney_seeds['Seed'].apply(lambda x : int(x[1:3]))\n",
    "    df.drop(columns={'Seed'}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(df_model):\n",
    "    # Select all columns except the results column \n",
    "    X = df_model.iloc[:, :-1]\n",
    "    # Select the results column\n",
    "    y = df_model.result\n",
    "\n",
    "    # Split the dataframe into test and train of 25% test size.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.31, random_state=42)\n",
    "    \n",
    "    clf = LogisticRegression(random_state=42)\n",
    "    grid = {'clf__C': np.logspace(start=-80, stop=40, num=50), 'clf__penalty': ['l1', 'l2'],}\n",
    "    \n",
    "    pipe = Pipeline([('clf', clf)])\n",
    "\n",
    "    # Instantiate grid search using 10-fold cross validation:\n",
    "    search = GridSearchCV(pipe, grid, cv=10, scoring='neg_log_loss')\n",
    "\n",
    "    # Learn relationship between predictors (basketball/tourney features) and outcome,\n",
    "    # and the best parameters for defining such:\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions on the test set, new data that haven't been introduced to the model:\n",
    "    predicted = search.predict(X_test)\n",
    "\n",
    "    # Predictions as probabilities:\n",
    "    probabilities = search.predict_proba(X_test)[:, 1]\n",
    "    # Accuracy scores for the training and test sets:\n",
    "    train_accuracy = search.score(X_train, y_train)\n",
    "    test_accuracy = search.score(X_test, y_test)\n",
    "\n",
    "#     print('Best Parameters: {}'.format(search.best_params_))\n",
    "#     print('Training Accuracy: {:0.2}'.format(train_accuracy))\n",
    "#     print('Test Accuracy: {:0.2}'.format(test_accuracy))\n",
    "    \n",
    "    # Confusion matrix labels:\n",
    "    labels = np.array([['true losses','false wins'], ['false losses','true wins']])\n",
    "\n",
    "    # Model evaluation metrics:\n",
    "    confusion_mtrx = confusion_matrix(y_test, predicted)\n",
    "    auc = roc_auc_score(y_test, probabilities)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, probabilities)\n",
    "    logloss = log_loss(y_test, search.predict_proba(X_test))\n",
    "\n",
    "#     print('logloss: {:0.7}'.format(logloss))\n",
    "    returnParams = search.best_params_\n",
    "    returnParams.update({'train_accuracy': train_accuracy})\n",
    "    returnParams.update({'test_accuracy': test_accuracy})\n",
    "    returnParams.update({'logloss': logloss})\n",
    "    returnParams.update({'clf': clf})\n",
    "    return returnParams\n",
    "    # Plot all metrics in a grid of subplots:\n",
    "#     fig = plt.figure(figsize=(12, 12))\n",
    "#     grid = plt.GridSpec(2, 4, wspace=0.75, hspace=0.5)\n",
    "\n",
    "    # Top-left plot - confusion matrix:\n",
    "#     plt.subplot(grid[0, :2])\n",
    "#     sns.heatmap(confusion_mtrx, annot=labels, fmt='')\n",
    "#     plt.xlabel('Predicted Games')\n",
    "#     plt.ylabel('Actual Games');\n",
    "\n",
    "#     # Top-right plot - ROC curve:\n",
    "#     plt.subplot(grid[0, 2:])\n",
    "#     plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "#     plt.plot(fpr, tpr, marker='.')\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     plt.title('AUROC: {:0.3}'.format(auc));\n",
    "\n",
    "#     # Bottom-left plot - support, or true predictions:\n",
    "#     plt.subplot(grid[1, :2])\n",
    "#     sns.countplot(y=predicted, orient='h')\n",
    "#     plt.yticks([1, 0], ('wins', 'losses'))\n",
    "#     plt.ylabel(''), plt.xlabel('Number Predicted');\n",
    "\n",
    "    # Bottom-right plot - classification report:\n",
    "    # plt.subplot(grid[1, 2:])\n",
    "    # visualizer = ClassificationReport(search, classes=['losses', 'wins'])\n",
    "    # visualizer.fit(X_train, y_train)\n",
    "    # visualizer.score(X_test, y_test)\n",
    "    # g = visualizer.poof();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModelDf(df_regularSeason_avgs, df_tourney_all, df_tourney_seeds):\n",
    "    df_features = df_regularSeason_avgs[['Season', 'TeamID', 'shoot_eff', 'score_op', 'off_rtg', 'def_rtg', 'sos', 'ie', 'efg_pct', 'to_poss', 'orb_pct', 'ft_rate', 'reb_pct', 'drb_pct', 'ts_pct', 'ast_rtio', 'blk_pct', 'stl_pct']]\n",
    "    # df_features.head() \n",
    "    df_features.dropna(inplace=True)\n",
    "    \n",
    "    # Merge seed data into Regular Season data\n",
    "    df_features = pd.merge(df_tourney_seeds, df_features, how='outer', left_on=['Season', 'TeamID'], right_on=['Season', 'TeamID'])\n",
    "    df_tourney = df_tourney_all[(df_tourney_all.Season >= 2003) & (df_tourney_all.Season < 2019)]\n",
    "    df_tourney.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Merge tournament winners season features:\n",
    "    df_winners = pd.merge(left=df_tourney[['Season', 'WTeamID', 'LTeamID']], right=df_features, how='left', left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'])\n",
    "    df_winners.drop(['TeamID'], inplace=True, axis=1) \n",
    "\n",
    "    # Merge tournament games with features from losing team:\n",
    "    df_losers = pd.merge(left=df_tourney[['Season', 'WTeamID', 'LTeamID']], right=df_features, how='left', left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'])\n",
    "    df_losers.drop(['TeamID'], inplace=True, axis=1)\n",
    "\n",
    "    # Creating a dataframe for the winning teams and assigning 1 to the result column to indiciate the win.\n",
    "    df_winner_diff = (df_winners.iloc[:, 3:] - df_losers.iloc[:, 3:])\n",
    "    df_winner_diff['result'] = 1\n",
    "\n",
    "    # Creating a dataframe for the losing teams and assigning 0 to the result column to indiciate the loss.\n",
    "    df_loser_diff = (df_losers.iloc[:, 3:] - df_winners.iloc[:, 3:])\n",
    "    df_loser_diff['result'] = 0\n",
    "\n",
    "    # Combine winning team data with losing team data\n",
    "    df_model = pd.concat((df_winner_diff, df_loser_diff), axis=0)\n",
    "    df_model.dropna(inplace=True)\n",
    "    return df_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Column names\n",
    "* WFGM - field goals made (by the winning team)\n",
    "* WFGA - field goals attempted (by the winning team)\n",
    "* WFGM3 - three pointers made (by the winning team)\n",
    "* WFGA3 - three pointers attempted (by the winning team)\n",
    "* WFTM - free throws made (by the winning team)\n",
    "* WFTA - free throws attempted (by the winning team)\n",
    "* WOR - offensive rebounds (pulled by the winning team)\n",
    "* WDR - defensive rebounds (pulled by the winning team)\n",
    "* WAst - assists (by the winning team)\n",
    "* WTO - turnovers committed (by the winning team)\n",
    "* WStl - steals (accomplished by the winning team)\n",
    "* WBlk - blocks (accomplished by the winning team)\n",
    "* WPF - personal fouls committed (by the winning team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2DataPath = 'data\\\\Stage2DataFiles\\\\'\n",
    "dataPath = 'data\\\\'\n",
    "# Read in detail tournment results from 2018 and previous\n",
    "df_tourneyDetailedResults = pd.read_csv(stage2DataPath + \"NCAATourneyDetailedResults.csv\")\n",
    "# Read in the team names and teamID\n",
    "df_regularSeason_teams = pd.read_csv( stage2DataPath + \"Teams.csv\")\n",
    "# Read in the regular season results\n",
    "df_regularSeason = pd.read_csv(stage2DataPath + \"RegularSeasonDetailedResults.csv\")\n",
    "# Read in the Tournment seed data\n",
    "df_tourney_seeds = pd.read_csv(stage2DataPath + \"NCAATourneySeeds.csv\")\n",
    "# Read in Conference information\n",
    "df_team_conf = pd.read_csv(stage2DataPath + 'TeamConferences.csv')\n",
    "df_conf = pd.read_csv(dataPath + \"Conferences.csv\")\n",
    "df_tourneyCompact = pd.read_csv(stage2DataPath + \"NCAATourneyCompactResults.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tourney_seeds = processSeeds(df_tourney_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_regularSeason.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping DayNum, WLoc, NumOT\n",
    "# df_regularSeason.drop(columns={'DayNum', 'WLoc','NumOT'}, inplace=True)\n",
    "# df_regularSeason.drop(columns={'NumOT'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildRegularSeason(df_regularSeason, df_conf, df_team_conf):\n",
    "    #     Merge team conference with conference\n",
    "    df_regularSeason_conf_name = df_team_conf.merge(df_conf, on=['ConfAbbrev'])\n",
    "\n",
    "    # Matching up winning and losing teamIDs\n",
    "    win_teams = df_regularSeason_teams.rename(columns={'TeamID':'WTeamID'})[['WTeamID', 'TeamName']]\n",
    "    win_confs = df_regularSeason_conf_name.rename(columns={'TeamID':'WTeamID'})[['Season', 'WTeamID', 'Description']]\n",
    "    lose_teams = df_regularSeason_teams.rename(columns={'TeamID':'LTeamID'})[['LTeamID', 'TeamName']]\n",
    "    lose_confs = df_regularSeason_conf_name.rename(columns={'TeamID':'LTeamID'})[['Season', 'LTeamID', 'Description']]\n",
    "\n",
    "    # Merge conference and team name of winning team and losing team\n",
    "    df_regularSeason = df_regularSeason.merge(win_teams, on='WTeamID').rename(columns={'TeamName': 'WTeamName'}) \\\n",
    "    .merge(win_confs, on=['Season', 'WTeamID']).rename(columns={'Description': 'WConfName'}) \\\n",
    "    .merge(lose_teams, on='LTeamID').rename(columns={'TeamName': 'LTeamName'}) \\\n",
    "    .merge(lose_confs, on=['Season', 'LTeamID']).rename(columns={'Description': 'LConfName'})\n",
    "\n",
    "    # Build two point Field goal information.\n",
    "    # Providing data includes 3 point shots in the field goal data, so FG - FG3 will get the two point field goal data.\n",
    "    df_regularSeason['WFGM2'] = df_regularSeason.WFGM - df_regularSeason.WFGM3\n",
    "    df_regularSeason['WFGA2'] = df_regularSeason.WFGA - df_regularSeason.WFGA3\n",
    "    df_regularSeason['LFGM2'] = df_regularSeason.LFGM - df_regularSeason.LFGM3\n",
    "    df_regularSeason['LFGA2'] = df_regularSeason.LFGA - df_regularSeason.LFGA3\n",
    "\n",
    "    # Winner stats related to offensive efficiency:\n",
    "    df_regularSeason['Wposs'] = df_regularSeason.apply(lambda row: row.WFGA + 0.475 * row.WFTA + row.WTO - row.WOR, axis=1)\n",
    "    df_regularSeason['Wshoot_eff'] = df_regularSeason.apply(lambda row: row.WScore / (row.WFGA + 0.475 * row.WFTA), axis=1)\n",
    "    df_regularSeason['Wscore_op'] = df_regularSeason.apply(lambda row: (row.WFGA + 0.475 * row.WFTA) / row.Wposs, axis=1)\n",
    "    df_regularSeason['Woff_rtg'] = df_regularSeason.apply(lambda row: row.WScore / row.Wposs*100, axis=1)\n",
    "\n",
    "    # Loser stats related to offensive efficiency:\n",
    "    df_regularSeason['Lposs'] = df_regularSeason.apply(lambda row: row.LFGA + 0.475 * row.LFTA + row.LTO - row.LOR, axis=1)\n",
    "    df_regularSeason['Lshoot_eff'] = df_regularSeason.apply(lambda row: row.LScore / (row.LFGA + 0.475 * row.LFTA), axis=1)\n",
    "    df_regularSeason['Lscore_op'] = df_regularSeason.apply(lambda row: (row.LFGA + 0.475 * row.LFTA) / row.Lposs, axis=1)\n",
    "    df_regularSeason['Loff_rtg'] = df_regularSeason.apply(lambda row: row.LScore/row.Lposs*100, axis=1)\n",
    "\n",
    "    # Defensive and net efficiency:\n",
    "    df_regularSeason['Wdef_rtg'] = df_regularSeason.apply(lambda row: row.Loff_rtg, axis=1)\n",
    "    df_regularSeason['Wsos'] = df_regularSeason.apply(lambda row: row.Woff_rtg - row.Loff_rtg, axis=1)\n",
    "    df_regularSeason['Ldef_rtg'] = df_regularSeason.apply(lambda row: row.Woff_rtg, axis=1)\n",
    "    df_regularSeason['Lsos'] = df_regularSeason.apply(lambda row: row.Loff_rtg - row.Woff_rtg, axis=1)\n",
    "\n",
    "    # Impact Estimate - \n",
    "    # First calculate the teams' overall statistical contribution (the numerator):\n",
    "    Wie = df_regularSeason.apply(lambda row: row.WScore + row.WFGM + row.WFTM - row.WFGA - row.WFTA + row.WDR + (0.5 * row.WOR) + row.WAst + row.WStl + (0.5 * row.WBlk) - row.WPF - row.WTO, axis=1)\n",
    "    Lie = df_regularSeason.apply(lambda row: row.LScore + row.LFGM + row.LFTM - row.LFGA - row.LFTA + row.LDR + (0.5 * row.LOR) + row.LAst + row.LStl + (0.5 * row.LBlk) - row.LPF - row.LTO, axis=1)\n",
    "\n",
    "    # Then divide by the total game statistics (the denominator):\n",
    "    df_regularSeason['Wie'] = Wie / (Wie + Lie) * 100\n",
    "    df_regularSeason['Lie'] = Lie / (Lie + Wie) * 100\n",
    "\n",
    "    # Other winner stats:\n",
    "    df_regularSeason['Wts_pct'] = df_regularSeason.apply(lambda row: row.WScore / (2 * (row.WFGA + 0.475 * row.WFTA)) * 100, axis=1)\n",
    "    df_regularSeason['Wefg_pct'] = df_regularSeason.apply(lambda row: (row.WFGM2 + 1.5 * row.WFGM3) / row.WFGA, axis=1)\n",
    "    df_regularSeason['Worb_pct'] = df_regularSeason.apply(lambda row: row.WOR / (row.WOR + row.LDR), axis=1)\n",
    "    df_regularSeason['Wdrb_pct'] = df_regularSeason.apply(lambda row: row.WDR / (row.WDR + row.LOR), axis=1)\n",
    "    df_regularSeason['Wreb_pct'] = df_regularSeason.apply(lambda row: (row.Worb_pct + row.Wdrb_pct) / 2, axis=1)\n",
    "    df_regularSeason['Wto_poss'] = df_regularSeason.apply(lambda row: row.WTO / row.Wposs, axis=1)\n",
    "    df_regularSeason['Wft_rate'] = df_regularSeason.apply(lambda row: row.WFTM / row.WFGA, axis=1)\n",
    "    df_regularSeason['Wast_rtio'] = df_regularSeason.apply(lambda row: row.WAst / (row.WFGA + 0.475*row.WFTA + row.WTO + row.WAst) * 100, axis=1)\n",
    "    df_regularSeason['Wblk_pct'] = df_regularSeason.apply(lambda row: row.WBlk / row.LFGA2 * 100, axis=1)\n",
    "    df_regularSeason['Wstl_pct'] = df_regularSeason.apply(lambda row: row.WStl / row.Lposs * 100, axis=1)\n",
    "\n",
    "    # Other loser stats:\n",
    "    df_regularSeason['Lts_pct'] = df_regularSeason.apply(lambda row: row.LScore / (2 * (row.LFGA + 0.475 * row.LFTA)) * 100, axis=1)\n",
    "    df_regularSeason['Lefg_pct'] = df_regularSeason.apply(lambda row: (row.LFGM2 + 1.5 * row.LFGM3) / row.LFGA, axis=1)\n",
    "    df_regularSeason['Lorb_pct'] = df_regularSeason.apply(lambda row: row.LOR / (row.LOR + row.WDR), axis=1)\n",
    "    df_regularSeason['Ldrb_pct'] = df_regularSeason.apply(lambda row: row.LDR / (row.LDR + row.WOR), axis=1)\n",
    "    df_regularSeason['Lreb_pct'] = df_regularSeason.apply(lambda row: (row.Lorb_pct + row.Ldrb_pct) / 2, axis=1)\n",
    "    df_regularSeason['Lto_poss'] = df_regularSeason.apply(lambda row: row.LTO / row.Lposs, axis=1)\n",
    "    df_regularSeason['Lft_rate'] = df_regularSeason.apply(lambda row: row.LFTM / row.LFGA, axis=1)\n",
    "    df_regularSeason['Last_rtio'] = df_regularSeason.apply(lambda row: row.LAst / (row.LFGA + 0.475*row.LFTA + row.LTO + row.LAst) * 100, axis=1)\n",
    "    df_regularSeason['Lblk_pct'] = df_regularSeason.apply(lambda row: row.LBlk / row.WFGA2 * 100, axis=1)\n",
    "    df_regularSeason['Lstl_pct'] = df_regularSeason.apply(lambda row: row.LStl / row.Wposs * 100, axis=1)\n",
    " \n",
    "    return df_regularSeason, win_teams, win_confs, lose_teams, lose_confs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTourneyCompact(df_tourneyCompact, df_tourney_seeds, win_teams, win_confs, lose_teams, lose_confs):\n",
    "    df_tourneyCompact['tourn_round'] = df_tourneyCompact.DayNum.apply(convertTourneyRound)\n",
    "\n",
    "#     df_tourney_seeds['seed'] = df_tourney_seeds['Seed'].apply(lambda x : int(x[1:3]))\n",
    "#     df_tourney_seeds.head()\n",
    "\n",
    "    # Drop Seed column\n",
    "#     df_tourney_seeds.drop(columns={'Seed'}, inplace=True)\n",
    "\n",
    "    # df_tourney_seeds.head()\n",
    "\n",
    "    df_tourneyCompact = df_tourneyCompact.merge(df_tourney_seeds, how='left', \n",
    "                                                            left_on=['Season', 'WTeamID'], \n",
    "                                                            right_on=['Season', 'TeamID']) \\\n",
    "    .rename(columns={'seed': 'Wseed'}).drop(['TeamID'], axis=1) \\\n",
    "    .merge(df_tourney_seeds, how='left', left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID']) \\\n",
    "    .rename(columns={'seed': 'Lseed'}).drop(['TeamID'], axis=1) \\\n",
    "    .merge(win_teams, on='WTeamID').rename(columns={'TeamName': 'WTeamName'}) \\\n",
    "    .merge(win_confs, on=['Season', 'WTeamID']).rename(columns={'Description': 'WConfName'}) \\\n",
    "    .merge(lose_teams, on='LTeamID').rename(columns={'TeamName': 'LTeamName'}) \\\n",
    "    .merge(lose_confs, on=['Season', 'LTeamID']).rename(columns={'Description': 'LConfName'})\n",
    "\n",
    "    # Calculate the point differential:\n",
    "    df_tourneyCompact['point_diff'] = df_tourneyCompact.WScore - df_tourneyCompact.LScore\n",
    "    return df_tourneyCompact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def buildRegSeasonStats(df_regularSeason):\n",
    "#     # Other winner stats:\n",
    "#     df_regularSeason['Wts_pct'] = df_regularSeason.apply(lambda row: row.WScore / (2 * (row.WFGA + 0.475 * row.WFTA)) * 100, axis=1)\n",
    "#     df_regularSeason['Wefg_pct'] = df_regularSeason.apply(lambda row: (row.WFGM2 + 1.5 * row.WFGM3) / row.WFGA, axis=1)\n",
    "#     df_regularSeason['Worb_pct'] = df_regularSeason.apply(lambda row: row.WOR / (row.WOR + row.LDR), axis=1)\n",
    "#     df_regularSeason['Wdrb_pct'] = df_regularSeason.apply(lambda row: row.WDR / (row.WDR + row.LOR), axis=1)\n",
    "#     df_regularSeason['Wreb_pct'] = df_regularSeason.apply(lambda row: (row.Worb_pct + row.Wdrb_pct) / 2, axis=1)\n",
    "#     df_regularSeason['Wto_poss'] = df_regularSeason.apply(lambda row: row.WTO / row.Wposs, axis=1)\n",
    "#     df_regularSeason['Wft_rate'] = df_regularSeason.apply(lambda row: row.WFTM / row.WFGA, axis=1)\n",
    "#     df_regularSeason['Wast_rtio'] = df_regularSeason.apply(lambda row: row.WAst / (row.WFGA + 0.475*row.WFTA + row.WTO + row.WAst) * 100, axis=1)\n",
    "#     df_regularSeason['Wblk_pct'] = df_regularSeason.apply(lambda row: row.WBlk / row.LFGA2 * 100, axis=1)\n",
    "#     df_regularSeason['Wstl_pct'] = df_regularSeason.apply(lambda row: row.WStl / row.Lposs * 100, axis=1)\n",
    "\n",
    "#     # Other loser stats:\n",
    "#     df_regularSeason['Lts_pct'] = df_regularSeason.apply(lambda row: row.LScore / (2 * (row.LFGA + 0.475 * row.LFTA)) * 100, axis=1)\n",
    "#     df_regularSeason['Lefg_pct'] = df_regularSeason.apply(lambda row: (row.LFGM2 + 1.5 * row.LFGM3) / row.LFGA, axis=1)\n",
    "#     df_regularSeason['Lorb_pct'] = df_regularSeason.apply(lambda row: row.LOR / (row.LOR + row.WDR), axis=1)\n",
    "#     df_regularSeason['Ldrb_pct'] = df_regularSeason.apply(lambda row: row.LDR / (row.LDR + row.WOR), axis=1)\n",
    "#     df_regularSeason['Lreb_pct'] = df_regularSeason.apply(lambda row: (row.Lorb_pct + row.Ldrb_pct) / 2, axis=1)\n",
    "#     df_regularSeason['Lto_poss'] = df_regularSeason.apply(lambda row: row.LTO / row.Lposs, axis=1)\n",
    "#     df_regularSeason['Lft_rate'] = df_regularSeason.apply(lambda row: row.LFTM / row.LFGA, axis=1)\n",
    "#     df_regularSeason['Last_rtio'] = df_regularSeason.apply(lambda row: row.LAst / (row.LFGA + 0.475*row.LFTA + row.LTO + row.LAst) * 100, axis=1)\n",
    "#     df_regularSeason['Lblk_pct'] = df_regularSeason.apply(lambda row: row.LBlk / row.WFGA2 * 100, axis=1)\n",
    "#     df_regularSeason['Lstl_pct'] = df_regularSeason.apply(lambda row: row.LStl / row.Wposs * 100, axis=1)\n",
    "#     df_regularSeason.reset_index(inplace = True)\n",
    "#     df_regularSeason = df_regularSeason.rename(columns={'Wshoot_eff':'shoot_eff', 'Wscore_op':'score_op', \n",
    "#                                     'Woff_rtg':'off_rtg', 'Wdef_rtg':'def_rtg', 'Wsos':'sos', \n",
    "#                                     'Wie':'ie', 'Wts_pct':'ts_pct', 'Wefg_pct':'efg_pct', 'Worb_pct':'orb_pct', \n",
    "#                                     'Wdrb_pct':'drb_pct', 'Wreb_pct':'reb_pct', 'Wto_poss':'to_poss', \n",
    "#                                     'Wft_rate':'ft_rate', 'Wast_rtio':'ast_rtio', \n",
    "#                                     'Wblk_pct':'blk_pct', 'Wstl_pct':'stl_pct'})\n",
    "#     df_regularSeason = df_regularSeason.rename(columns={'WTeamID': 'TeamID', 'WTeamName': 'TeamName', 'WConfName': 'ConfName'})\n",
    "#     return df_regularSeason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regularSeason, win_teams, win_confs, lose_teams, lose_confs = buildRegularSeason(df_regularSeason, df_conf, df_team_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tourney_merge = buildTourneyCompact(df_tourneyCompact, df_tourney_seeds, win_teams, win_confs, lose_teams, lose_confs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tourney_all = df_tourneyCompact[df_tourneyCompact.Season < 2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations \n",
    "weights = [x / 100.0 for x in range(0, 105, 5)]\n",
    "weightPerms = permutations(weights, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dayWeight: 0.0, location Weight: 0.1, logloss: 0.5342795540424871, Test Acc: -0.5342795540424871, Train Acc: -0.5418989536926416\n",
      "dayWeight: 0.0, location Weight: 0.15, logloss: 0.5342487176294132, Test Acc: -0.5342487176294132, Train Acc: -0.5418909005056215\n",
      "dayWeight: 0.0, location Weight: 0.2, logloss: 0.5343981231232436, Test Acc: -0.5343981231232436, Train Acc: -0.5417543851604567\n",
      "dayWeight: 0.0, location Weight: 0.25, logloss: 0.5343057646662144, Test Acc: -0.5343057646662144, Train Acc: -0.5417778100682377\n",
      "dayWeight: 0.0, location Weight: 0.3, logloss: 0.5342538408305044, Test Acc: -0.5342538408305044, Train Acc: -0.541874176215571\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-5924999b0a74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mdf_regularSeason_weighted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'WTeamID'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'TeamID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'WTeamName'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'TeamName'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'WConfName'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'ConfName'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mdf_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuildModelDf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_regularSeason_weighted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_tourney_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_tourney_seeds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mtempDict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuildModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mtempDict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"dayWeight\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdayWeight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"locWeight\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlocWeight\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     print(\"dayWeight: {}, location Weight: {}, logloss: {}, Test Acc: {}, Train Acc: {}\".format(tempDict['dayWeight'],tempDict['locWeight'],\n",
      "\u001b[1;32m<ipython-input-34-6d6331b9729e>\u001b[0m in \u001b[0;36mbuildModel\u001b[1;34m(df_model)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# Learn relationship between predictors (basketball/tourney features) and outcome,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# and the best parameters for defining such:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0msearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# Predictions on the test set, new data that haven't been introduced to the model:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tecmmx\\desktop\\code\\udacity\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    720\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tecmmx\\desktop\\code\\udacity\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tecmmx\\desktop\\code\\udacity\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    709\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 711\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tecmmx\\desktop\\code\\udacity\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 920\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    921\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tecmmx\\desktop\\code\\udacity\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tecmmx\\desktop\\code\\udacity\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tecmmx\\desktop\\code\\udacity\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tecmmx\\desktop\\code\\udacity\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tecmmx\\desktop\\code\\udacity\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tecmmx\\desktop\\code\\udacity\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tecmmx\\desktop\\code\\udacity\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tecmmx\\desktop\\code\\udacity\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tecmmx\\desktop\\code\\udacity\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1303\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1305\u001b[1;33m                 sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m   1306\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tecmmx\\desktop\\code\\udacity\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m    921\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 923\u001b[1;33m         epsilon, sample_weight)\n\u001b[0m\u001b[0;32m    924\u001b[0m     \u001b[1;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m     \u001b[1;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf_results = []\n",
    "for weight in weightPerms:\n",
    "    tempDict = {}\n",
    "    dayWeight,locWeight = weight\n",
    "    \n",
    "    df_regularSeason_weighted = df_regularSeason.apply(weighStats, axis=1, args=(dayWeight, locWeight))\n",
    "\n",
    "    df_regularSeason_weighted.reset_index(inplace = True)\n",
    "    df_regularSeason_weighted.rename(columns={'Wshoot_eff':'shoot_eff', 'Wscore_op':'score_op', \n",
    "                                    'Woff_rtg':'off_rtg', 'Wdef_rtg':'def_rtg', 'Wsos':'sos', \n",
    "                                    'Wie':'ie', 'Wts_pct':'ts_pct', 'Wefg_pct':'efg_pct', 'Worb_pct':'orb_pct', \n",
    "                                    'Wdrb_pct':'drb_pct', 'Wreb_pct':'reb_pct', 'Wto_poss':'to_poss', \n",
    "                                    'Wft_rate':'ft_rate', 'Wast_rtio':'ast_rtio', \n",
    "                                    'Wblk_pct':'blk_pct', 'Wstl_pct':'stl_pct'}, inplace=True)\n",
    "    df_regularSeason_weighted.rename(columns={'WTeamID': 'TeamID', 'WTeamName': 'TeamName', 'WConfName': 'ConfName'}, inplace=True)\n",
    "    df_model = buildModelDf(df_regularSeason_weighted, df_tourney_all, df_tourney_seeds)\n",
    "    tempDict = buildModel(df_model)\n",
    "    tempDict.update({\"dayWeight\":dayWeight, \"locWeight\":locWeight})\n",
    "    clf_results.append(tempDict)\n",
    "    print(\"dayWeight: {}, location Weight: {}, logloss: {}, Test Acc: {}, Train Acc: {}\".format(tempDict['dayWeight'],tempDict['locWeight'],\n",
    "                                                     tempDict['logloss'],tempDict['test_accuracy'],tempDict['train_accuracy']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_regularSeason_avgs = buildRegSeasonStats(df_regularSeason_weighted)\n",
    "# df_regularSeason_weighted.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(20,15))\n",
    "# y_bar = df_regularSeason[\"WConfName\"].value_counts().tolist()\n",
    "# x_bar = df_regularSeason[\"WConfName\"].value_counts().keys()\n",
    "# ax.barh(x_bar, y_bar, align='center',\n",
    "#         color='green', ecolor='black')\n",
    "# # ax.set_yticks(y_bar)\n",
    "# # ax.set_yticklabels(x_bar)\n",
    "# ax.invert_yaxis()  # labels read top-to-bottom\n",
    "# ax.set_xlabel('Wins')\n",
    "# ax.set_ylabel('Conference')\n",
    "# ax.set_title('Number of NCAA Division I wins per conference')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # temp = df_regularSeason_regularSeason.Season.value_counts().tolist()\n",
    "# y_bar = df_regularSeason[\"Season\"].value_counts().tolist()\n",
    "# x_bar = df_regularSeason[\"Season\"].value_counts().keys()\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(15,5))\n",
    "# ax.barh(x_bar, y_bar, align='center',\n",
    "#         color='green', ecolor='black')\n",
    "# # ax.set_yticks(y_bar)\n",
    "# # ax.set_yticklabels(x_bar)\n",
    "# # ax.invert_yaxis()  # labels read top-to-bottom\n",
    "# ax.set_xlabel('Games')\n",
    "# ax.set_ylabel('Season')\n",
    "# ax.set_title('Number of Games per Season')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shooting Efficiency = Score / ( FGA + 0.475 * FTA )\n",
    "# Scoring Opportunity = (FGA + 0.475 FTA)/(poss)\n",
    "# Defensive Rating = opponent's offensive rating\n",
    "# Offensive Rating = (score / poss) * 100\n",
    "# Net Efficiency = offensive rating - opponent offensive rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shooting features\n",
    "# True Shooting percentage = ((score)/(2 * (FGA + 0.475 * FTA))) * 100\n",
    "# Effective Field Goal Percentage = (FGM2 + 1.5 * FGM3) / (FGA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebounding features\n",
    "# Offensive Rebound Percentage(orb_pct) = (OR) / (OR + opponent DR)\n",
    "# Defensive Rebound Percentage(drb_pct) = (DR) / (DR + opponent OR)\n",
    "# Rebound Percentage(reb_pct) = (orb_pct + drb_pct)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basketball on Paper author Dean Oliver outlines four factors that determine success in basketball:\n",
    "# Effective Field Goal percentage\n",
    "# Turnovers per possession(to_poss) = Turnovers / poss\n",
    "# Offensive Rebound Percentage\n",
    "# Free throw rate(ft_rate) = FTM / FGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stats.nba.com/help/faq/ - NBA Advanced Stats page\n",
    "# Team Impact Estimate\n",
    "# IE_numerator = Score + FGM + FTM - FGA - FTA + DR + 0.5 * OR + Ast + Stl + 0.5 * Blk - PF - TO\n",
    "# IE = IE_numerator / (IE_Numerator + opp_IE_numerator)\n",
    "\n",
    "# Assist Ratio(ast_rat) = (Ast / (FGA + 0.475 * FTA + TO + Ast)) * 100\n",
    "# Block Percentage(blk_pct) = (Blk / opp_FGA2) * 100\n",
    "# Steal Percentage(stl_pct) = (Stl / opp_poss) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_regularSeason.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_regularSeason_weighted = df_regularSeason.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_regularSeason_weighted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_regularSeason_avgs = calcAverages(df=df_regularSeason_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_regularSeason_weighted[(df_regularSeason_weighted['WTeamID'] == 1104) & (df_regularSeason_weighted['DayNum'] <= 99) & \n",
    "#                               (df_regularSeason_weighted['Season'] == 2003)]\n",
    "# .groupby(df_regularSeason_weighted['Season']).sum()\n",
    "# \n",
    "# temp_df.Lts_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weighted average using win percentage:\n",
    "# df_regularSeason_avgs['shoot_eff'] = df_regularSeason_avgs['Wshoot_eff'] * df_regularSeason_avgs['win_pct'] + df_regularSeason_avgs['Lshoot_eff'] * (1 - df_regularSeason_avgs['win_pct'])\n",
    "# df_regularSeason_avgs['score_op'] = df_regularSeason_avgs['Wscore_op'] * df_regularSeason_avgs['win_pct'] + df_regularSeason_avgs['Lscore_op'] * (1 - df_regularSeason_avgs['win_pct'])\n",
    "# df_regularSeason_avgs['off_rtg'] = df_regularSeason_avgs['Woff_rtg'] * df_regularSeason_avgs['win_pct'] + df_regularSeason_avgs['Loff_rtg'] * (1 - df_regularSeason_avgs['win_pct'])\n",
    "# df_regularSeason_avgs['def_rtg'] = df_regularSeason_avgs['Wdef_rtg'] * df_regularSeason_avgs['win_pct'] + df_regularSeason_avgs['Ldef_rtg'] * (1 - df_regularSeason_avgs['win_pct'])\n",
    "# df_regularSeason_avgs['sos'] = df_regularSeason_avgs['Wsos'] * df_regularSeason_avgs['win_pct'] + df_regularSeason_avgs['Lsos'] * (1 - df_regularSeason_avgs['win_pct'])\n",
    "# df_regularSeason_avgs['ts_pct'] = df_regularSeason_avgs['Wts_pct'] * df_regularSeason_avgs['win_pct'] + df_regularSeason_avgs['Lts_pct'] * (1 - df_regularSeason_avgs['win_pct'])\n",
    "# df_regularSeason_avgs['efg_pct'] = df_regularSeason_avgs['Wefg_pct'] * df_regularSeason_avgs['win_pct'] + df_regularSeason_avgs['Lefg_pct'] * (1 - df_regularSeason_avgs['win_pct'])\n",
    "# df_regularSeason_avgs['orb_pct'] = df_regularSeason_avgs['Worb_pct'] * df_regularSeason_avgs['win_pct'] + df_regularSeason_avgs['Lorb_pct'] * (1 - df_regularSeason_avgs['win_pct'])\n",
    "# df_regularSeason_avgs['drb_pct'] = df_regularSeason_avgs['Wdrb_pct'] * df_regularSeason_avgs['win_pct'] + df_regularSeason_avgs['Ldrb_pct'] * (1 - df_regularSeason_avgs['win_pct'])\n",
    "# df_regularSeason_avgs['reb_pct'] = df_regularSeason_avgs['Wreb_pct'] * df_regularSeason_avgs['win_pct'] + df_regularSeason_avgs['Lreb_pct'] * (1 - df_regularSeason_avgs['win_pct'])\n",
    "# df_regularSeason_avgs['to_poss'] = df_regularSeason_avgs['Wto_poss'] * df_regularSeason_avgs['win_pct'] + df_regularSeason_avgs['Lto_poss'] * (1 - df_regularSeason_avgs['win_pct'])\n",
    "# df_regularSeason_avgs['ft_rate'] = df_regularSeason_avgs['Wft_rate'] * df_regularSeason_avgs['win_pct'] + df_regularSeason_avgs['Lft_rate'] * (1 - df_regularSeason_avgs['win_pct'])\n",
    "# df_regularSeason_avgs['ie'] = df_regularSeason_avgs['Wie'] * df_regularSeason_avgs['win_pct'] + df_regularSeason_avgs['Lie'] * (1 - df_regularSeason_avgs['win_pct'])\n",
    "# df_regularSeason_avgs['ast_rtio'] = df_regularSeason_avgs['Wast_rtio'] * df_regularSeason_avgs['win_pct'] + df_regularSeason_avgs['Last_rtio'] * (1 - df_regularSeason_avgs['win_pct'])\n",
    "# df_regularSeason_avgs['blk_pct'] = df_regularSeason_avgs['Wblk_pct'] * df_regularSeason_avgs['win_pct'] + df_regularSeason_avgs['Lblk_pct'] * (1 - df_regularSeason_avgs['win_pct'])\n",
    "# df_regularSeason_avgs['stl_pct'] = df_regularSeason_avgs['Wstl_pct'] * df_regularSeason_avgs['win_pct'] + df_regularSeason_avgs['Lstl_pct'] * (1 - df_regularSeason_avgs['win_pct'])\n",
    "\n",
    "# df_regularSeason_avgs.reset_index(inplace = True)\n",
    "# df_regularSeason_avgs = df_regularSeason_avgs.rename(columns={'WTeamID': 'TeamID', 'WTeamName': 'TeamName', 'WConfName': 'ConfName'})\n",
    "# df_regularSeason_avgs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tourneyCompact()\n",
    "# df_tourneyCompact.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tourney_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# championships = df_tourney_all[df_tourney_all.tourn_round == 2]\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# sns.countplot(y=championships.WTeamName, order=championships.WTeamName.value_counts().index, color='#3c7f99')\n",
    "# plt.box(False)\n",
    "\n",
    "# fig.text(x=-0.05, y=0.95, s='       Championships per Team since 1985       ', fontsize=32, weight='bold', color='white', backgroundcolor='#c5b783')\n",
    "# plt.title('(When the tournament field expanded to 64 teams.)', fontsize=18)\n",
    "\n",
    "# plt.tick_params(axis='both', which='both',length=0)\n",
    "# plt.tick_params(axis='both', which='major', labelsize=16)\n",
    "# ax.xaxis.grid(which='both', linewidth=0.5, color='#3c7f99')\n",
    "# plt.xlabel(''), plt.ylabel('');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# sns.countplot(x=df_tourney_all[df_tourney_all.tourn_round == 64].Wseed, color='#3c7f99')\n",
    "# plt.box(False)\n",
    "\n",
    "# fig.text(x=0, y=0.95, s='       First Round Wins by Seed since 1985     ', fontsize=32, weight='bold', color='white', backgroundcolor='#c5b783')\n",
    "# plt.title('(When the tournament field expanded to 64 teams.)', fontsize=18)\n",
    "\n",
    "# plt.tick_params(axis='both', which='both',length=0)\n",
    "# plt.tick_params(axis='both', which='major', labelsize=16)\n",
    "# ax.yaxis.grid(which='both', linewidth=0.5, color='#3c7f99')\n",
    "# plt.xlabel(''), plt.ylabel('');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the correlation matrix:\n",
    "# matrix = df_regularSeason_avgs[['win_pct', 'shoot_eff', 'score_op', 'off_rtg', 'def_rtg', 'sos', 'ie', 'ts_pct', 'efg_pct', 'orb_pct', 'drb_pct', 'reb_pct', 'to_poss', 'ft_rate', 'ast_rtio', 'blk_pct', 'stl_pct']].corr()\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(15,15))         # Sample figsize in inches\n",
    "# # sns.heatmap(df1.iloc[:, 1:6:], annot=True, linewidths=.5, ax=ax)\n",
    "\n",
    "# # Create mask for the upper triangle:\n",
    "# mask = np.zeros_like(matrix, dtype=np.bool)\n",
    "# mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# # Create a custom diverging colormap:\n",
    "# cmap = sns.diverging_palette(225, 45, as_cmap=True)\n",
    "\n",
    "# sns.heatmap(matrix, mask=mask, cmap=cmap, center=0, annot=True, square=True, linewidths=0.25, cbar_kws={'shrink': 0.25}, ax=ax)\n",
    "# plt.tick_params(axis='both', which='both',length=0)\n",
    "# plt.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "# from scipy.stats.stats import pearsonr\n",
    "# plt.title('Team Impact Estimate and Win Percentage correlate at an R square of {:0.3f}!'.format(pearsonr(df_regularSeason_avgs.ie, df_regularSeason_avgs.win_pct)[0]), fontsize=24, weight='bold')\n",
    "# # fig.text(x=0.25, y=0.75, s='In fact, Win Percentage has a moderate or strong positive \\nrelationship with most of the efficiency measures.', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTES \n",
    "# https://www.kaggle.com/humburgc/history-eda-machine-learning-march-madness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KeyError: \"['TeamID' 'shoot_eff' 'score_op' 'off_rtg' 'def_rtg' 'sos' 'ie' 'efg_pct'\\n 'to_poss' 'orb_pct'\n",
    "# 'ft_rate' 'reb_pct' 'drb_pct' 'ts_pct' 'ast_rtio'\\n 'blk_pct' 'stl_pct'] not in index\"\n",
    "# df_regularSeason_weighted.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_features.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tourney_seeds.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_features.head()\n",
    "# df_features.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_features.head()\n",
    "# df_features.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_features[df_features.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tourney.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_tourney_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_features = df_regularSeason_weighted[['Season', 'TeamID', 'shoot_eff', 'score_op', 'off_rtg', 'def_rtg', 'sos', 'ie', 'efg_pct', 'to_poss', 'orb_pct', 'ft_rate', 'reb_pct', 'drb_pct', 'ts_pct', 'ast_rtio', 'blk_pct', 'stl_pct']]\n",
    "# # df_features.head() \n",
    "# # Merge seed data into Regular Season data\n",
    "# df_features = pd.merge(df_tourney_seeds, df_features, how='outer', left_on=['Season', 'TeamID'], right_on=['Season', 'TeamID'])\n",
    "# df_tourney = df_tourney_all[(df_tourney_all.Season >= 2003) & (df_tourney_all.Season < 2019)]\n",
    "# df_tourney.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# # Merge tournament winners season features:\n",
    "# df_winners = pd.merge(left=df_tourney[['Season', 'WTeamID', 'LTeamID']], right=df_features, how='left', left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'])\n",
    "# df_winners.iloc[:, 3:] \n",
    "# df_winners.drop(['TeamID','Seed'], inplace=True, axis=1) \n",
    "\n",
    "# # Merge tournament games with features from losing team:\n",
    "# df_losers = pd.merge(left=df_tourney[['Season', 'WTeamID', 'LTeamID']], right=df_features, how='left', left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'])\n",
    "# df_losers.drop(['TeamID'], inplace=True, axis=1)\n",
    "\n",
    "# # # Creating a dataframe for the winning teams and assigning 1 to the result column to indiciate the win.\n",
    "# df_winner_diff = (df_winners.iloc[:, 3:] - df_losers.iloc[:, 3:])\n",
    "# df_winner_diff['result'] = 1\n",
    "\n",
    "# # # Creating a dataframe for the losing teams and assigning 0 to the result column to indiciate the loss.\n",
    "# # df_loser_diff = (df_losers.iloc[:, 3:] - df_winners.iloc[:, 3:])\n",
    "# # df_loser_diff['result'] = 0\n",
    "\n",
    "# # # Combine winning team data with losing team data\n",
    "# # df_model = pd.concat((df_winner_diff, df_loser_diff), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_regularSeason_weighted.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_model.columns\n",
    "# df_model.drop(columns={'seed'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_model[df_model.isnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.countplot(y)\n",
    "# plt.xlabel(''), plt.ylabel('')\n",
    "# plt.xticks([0, 1], ('losses', 'wins'))\n",
    "# plt.title('The Target Classes are Balanced');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('clf', log_clf)])\n",
    "    \n",
    "lr_search = GridSearchCV(pipe, log_param_grid, cv=10)\n",
    "lr_search.fit(X_train, y_train)\n",
    "print(\"Best C: {}; Best penalty: {}\".format(lr_search.best_params_['clf__C'], lr_search.best_params_['clf__penalty']))\n",
    "\n",
    "perm = PermutationImportance(lr_search, random_state=42).fit(X_test, y_test)\n",
    "eli5.show_weights(perm, feature_names=X_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict_pre2019 = pd.read_csv(dataPath + 'SampleSubmissionStage1.csv')\n",
    "df_predictions = pd.read_csv(dataPath + 'SampleSubmissionStage2.csv')\n",
    "df_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2019\n",
    "predictMatches(lr_search, df_predictions, df_features, year).to_csv('best_model_results_{}.csv'.format(year), index=False)\n",
    "predictMatches(lr_search, df_predict_pre2019, df_features, year).to_csv('best_model_results_pre{}.csv'.format(year), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bracketeer import build_bracket\n",
    "\n",
    "b = build_bracket(\n",
    "        outputPath='best_bracket_{}.png'.format(year),\n",
    "        submissionPath='best_model_results_2019.csv',\n",
    "        teamsPath= stage2DataPath + 'Teams.csv',\n",
    "        seedsPath= stage2DataPath + 'NCAATourneySeeds.csv',\n",
    "        slotsPath= stage2DataPath + 'NCAATourneySlots.csv',\n",
    "        year=year\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2019 Predictions\n",
    "from IPython.display import HTML, Image\n",
    "import random\n",
    "__counter__ = random.randint(0,2e9)\n",
    "\n",
    "Image(url=\"best_bracket_2019.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tourneyCompact\n",
    "df_tc = pd.read_csv(stage2DataPath + \"NCAATourneyCompactResults.csv\")\n",
    "df_tc['ID'] = df_tc['Season'].astype(str) + '_' \\\n",
    "              + (np.minimum(df_tc['WTeamID'],df_tc['LTeamID'])).astype(str) + '_' \\\n",
    "              + (np.maximum(df_tc['WTeamID'],df_tc['LTeamID'])).astype(str)\n",
    "\n",
    "df_tc['Result'] = 1*(df_tc['WTeamID'] < df_tc['LTeamID'])\n",
    "# df_tourneyCompact.head(10)\n",
    "\n",
    "df_tc = df_tc.merge(df_tourney_seeds.rename(columns={'Seed':'WSeed','TeamID':'WTeamID'}), \n",
    "                    how='inner', on=['Season', 'WTeamID'])\n",
    "df_tc = df_tc.merge(df_tourney_seeds.rename(columns={'Seed':'LSeed','TeamID':'LTeamID'}), \n",
    "                    how='inner', on=['Season', 'LTeamID'])\n",
    "# Remove any play-in games\n",
    "# df_playin = df_tc[df_tc['WSeed'].str[0:3] == df_tc['LSeed'].str[0:3]]\n",
    "df_tc.head(10)\n",
    "# df_tc = df_tc[df_tc['WSeed'].str[0:3] != df_tc['LSeed'].str[0:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.read_csv(\"best_model_results_pre2019.csv\")\n",
    "# df_submission.head()\n",
    "df_score, df_analysis = score_submission(df_submission, df_tc, on_season = np.arange(2014,2019), return_df_analysis=True)\n",
    "df_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score['LogLoss'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
